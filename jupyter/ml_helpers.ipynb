{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Split the data into train and test sets\n",
    "def train_test_split(X, y, test_size=0.2):\n",
    "\t# First, shuffle the data\n",
    "    train_data, train_labels = shuffle_data(X, y)\n",
    "\n",
    "    # Split the training data from test data in the ratio specified in test_size\n",
    "    split_i = len(y) - int(len(y) // (1 / test_size))\n",
    "    x_train, x_test = train_data[:split_i], train_data[split_i:]\n",
    "    y_train, y_test = train_labels[:split_i], train_labels[split_i:]\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "# Randomly shuffle the data\n",
    "def shuffle_data(data, labels):\n",
    "\tif(len(data) != len(labels)):\n",
    "\t\traise Exception(\"The given data and labels do NOT have the same length\")\n",
    "\n",
    "\tcombined = list(zip(data, labels))\n",
    "\trandom.shuffle(combined)\n",
    "\tdata[:], labels[:] = zip(*combined)\n",
    "\treturn data, labels\n",
    "\n",
    "# Calculate the distance between two vectors\n",
    "def euclidean_distance(vec_1, vec_2):\n",
    "\tif(len(vec_1) != len(vec_2)):\n",
    "\t\traise Exception(\"The two vectors do NOT have equal length\")\n",
    "\n",
    "\tdistance = 0\n",
    "\tfor i in range(len(vec_1)):\n",
    "\t\tdistance += pow((vec_1[i] - vec_2[i]), 2)\n",
    "\n",
    "\treturn np.sqrt(distance)\n",
    "\n",
    "# Compute the mean and variance of each feature of a data set\n",
    "def compute_mean_and_var(data):\n",
    "\tnum_elements = len(data)\n",
    "\ttotal = [0] * data.shape[1]\n",
    "\tfor sample in data:\n",
    "\t\ttotal = total + sample\n",
    "\tmean_features = np.divide(total, num_elements)\n",
    "\n",
    "\ttotal = [0] * data.shape[1]\n",
    "\tfor sample in data:\n",
    "\t\ttotal = total + np.square(sample - mean_features)\n",
    "\n",
    "\tstd_features = np.divide(total, num_elements)\n",
    "\n",
    "\tvar_features = std_features ** 2\n",
    "\n",
    "\treturn mean_features, var_features\n",
    "\n",
    "# Normalize data by subtracting mean and dividing by standard deviation\n",
    "def normalize_data(data):\n",
    "\tmean_features, var_features = compute_mean_and_var(data)\n",
    "\tstd_features = np.sqrt(var_features)\n",
    "\n",
    "\tfor index, sample in enumerate(data):\n",
    "\t\tdata[index] = np.divide((sample - mean_features), std_features) \n",
    "\n",
    "\treturn data\n",
    "\n",
    "# Divide dataset based on if sample value on feature index is larger than\n",
    "# the given threshold\n",
    "def divide_on_feature(X, feature_i, threshold):\n",
    "    split_func = None\n",
    "    if isinstance(threshold, int) or isinstance(threshold, float):\n",
    "        split_func = lambda sample: sample[feature_i] >= threshold\n",
    "    else:\n",
    "        split_func = lambda sample: sample[feature_i] == threshold\n",
    "\n",
    "    X_1 = np.array([sample for sample in X if split_func(sample)])\n",
    "    X_2 = np.array([sample for sample in X if not split_func(sample)])\n",
    "\n",
    "    return np.array([X_1, X_2])\n",
    "\n",
    "# Return random subsets (with replacements) of the data\n",
    "def get_random_subsets(X, y, n_subsets, replacements=True):\n",
    "    n_samples = np.shape(X)[0]\n",
    "    # Concatenate x and y and do a random shuffle\n",
    "    X_y = np.concatenate((X, y.reshape((1, len(y))).T), axis=1)\n",
    "    np.random.shuffle(X_y)\n",
    "    subsets = []\n",
    "\n",
    "    # Uses 50% of training samples without replacements\n",
    "    subsample_size = n_samples // 2\n",
    "    if replacements:\n",
    "        subsample_size = n_samples      # 100% with replacements\n",
    "\n",
    "    for _ in range(n_subsets):\n",
    "        idx = np.random.choice(range(n_samples), size=np.shape(range(subsample_size)), replace=replacements)\n",
    "        X = X_y[idx][:, :-1]\n",
    "        y = X_y[idx][:, -1]\n",
    "        subsets.append([X, y])\n",
    "    return subsets\n",
    "\n",
    "# Calculate the entropy of label array y\n",
    "def calculate_entropy(y):\n",
    "    log2 = lambda x: np.log(x) / np.log(2)\n",
    "    unique_labels = np.unique(y)\n",
    "    entropy = 0\n",
    "    for label in unique_labels:\n",
    "        count = len(y[y == label])\n",
    "        p = count / len(y)\n",
    "        entropy += -p * log2(p)\n",
    "    return entropy\n",
    "\n",
    "# Returns the mean squared error between y_true and y_pred\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    mse = np.mean(np.power(y_true - y_pred, 2))\n",
    "    return mse\n",
    "\n",
    "# The sigmoid function\n",
    "def sigmoid(val):\n",
    "\treturn np.divide(1, (1 + np.exp(-1*val)))\n",
    "\n",
    "# The derivative of the sigmoid function\n",
    "def sigmoid_gradient(val):\n",
    "    return sigmoid(val) * (1 - sigmoid(val))\n",
    "\n",
    "# Compute the covariance matrix of an array\n",
    "def compute_cov_mat(data):\n",
    "\t# Compute the mean of the data\n",
    "\tmean_vec = np.mean(data, axis=0)\n",
    "\n",
    "\t# Compute the covariance matrix\n",
    "\tcov_mat = (data - mean_vec).T.dot((data - mean_vec)) / (data.shape[0]-1)\n",
    "\n",
    "\treturn cov_mat\n",
    "\n",
    "\n",
    "# Perform PCA dimensionality reduction\n",
    "def pca(data, exp_var_percentage=95):\n",
    "\n",
    "\t# Compute the covariance matrix\n",
    "\tcov_mat = compute_cov_mat(data)\n",
    "\n",
    "\t# Compute the eigen values and vectors of the covariance matrix\n",
    "\teig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
    "\n",
    "\t# Make a list of (eigenvalue, eigenvector) tuples\n",
    "\teig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "\n",
    "\t# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "\teig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "\t# Only keep a certain number of eigen vectors based on the \"explained variance percentage\"\n",
    "\t# which tells us how much information (variance) can be attributed to each of the principal components\n",
    "\ttot = sum(eig_vals)\n",
    "\tvar_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]\n",
    "\tcum_var_exp = np.cumsum(var_exp)\n",
    "\n",
    "\tnum_vec_to_keep = 0\n",
    "\n",
    "\tfor index, percentage in enumerate(cum_var_exp):\n",
    "\t\tif percentage > exp_var_percentage:\n",
    "\t\t\tnum_vec_to_keep = index + 1\n",
    "\t\t\tbreak\n",
    "\n",
    "\t# Compute the projection matrix based on the top eigen vectors\n",
    "\tproj_mat = eig_pairs[0][1].reshape(4,1)\n",
    "\tfor eig_vec_idx in range(1, num_vec_to_keep):\n",
    "\t\tproj_mat = np.hstack((proj_mat, eig_pairs[eig_vec_idx][1].reshape(4,1)))\n",
    "\n",
    "\t# Project the data \n",
    "\tpca_data = data.dot(proj_mat)\n",
    "\n",
    "\treturn pca_data\n",
    "\n",
    "# 1D Gaussian Function\n",
    "def gaussian_1d(val, mean, standard_dev):\n",
    "\tcoeff = 1 / (standard_dev * np.sqrt(2 * np.pi))\n",
    "\texponent = (-1 * (val - mean) ** 2) / (2 * (standard_dev ** 2))\n",
    "\tgauss = coeff * np.exp(exponent)\n",
    "\treturn gauss\n",
    "\n",
    "# 2D Gaussian Function\n",
    "def gaussian_2d(x_val, y_val, x_mean, y_mean, x_standard_dev, y_standard_dev):\n",
    "\tx_gauss = gaussian_1d(x_val, x_mean, x_standard_dev)\n",
    "\ty_gauss = gaussian_1d(y_val, y_mean, y_standard_dev)\n",
    "\tgauss = x_gauss * y_gauss\n",
    "\treturn gauss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
