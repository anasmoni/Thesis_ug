{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import random\n",
    "\n",
    "                                         #function definetions starts from  here\n",
    "# Split the data into train and test sets\n",
    "def train_test_split(X, y, test_size=0.2):\n",
    "\t# First, shuffle the data\n",
    "    train_data, train_labels = shuffle_data(X, y)\n",
    "\n",
    "    # Split the training data from test data in the ratio specified in test_size\n",
    "    split_i = len(y) - int(len(y) // (1 / test_size))\n",
    "    x_train, x_test = train_data[:split_i], train_data[split_i:]\n",
    "    y_train, y_test = train_labels[:split_i], train_labels[split_i:]\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "# Randomly shuffle the data\n",
    "def shuffle_data(data, labels):\n",
    "\tif(len(data) != len(labels)):\n",
    "\t\traise Exception(\"The given data and labels do NOT have the same length\")\n",
    "\n",
    "\tcombined = list(zip(data, labels))\n",
    "\trandom.shuffle(combined)\n",
    "\tdata[:], labels[:] = zip(*combined)\n",
    "\treturn data, labels\n",
    "\n",
    "# Calculate the distance between two vectors\n",
    "def euclidean_distance(vec_1, vec_2):\n",
    "\tif(len(vec_1) != len(vec_2)):\n",
    "\t\traise Exception(\"The two vectors do NOT have equal length\")\n",
    "\n",
    "\tdistance = 0\n",
    "\tfor i in range(len(vec_1)):\n",
    "\t\tdistance += pow((vec_1[i] - vec_2[i]), 2)\n",
    "\n",
    "\treturn np.sqrt(distance)\n",
    "\n",
    "# Compute the mean and variance of each feature of a data set\n",
    "def compute_mean_and_var(data):\n",
    "\tnum_elements = len(data)\n",
    "\ttotal = [0] * data.shape[1]\n",
    "\tfor sample in data:\n",
    "\t\ttotal = total + sample\n",
    "\tmean_features = np.divide(total, num_elements)\n",
    "\n",
    "\ttotal = [0] * data.shape[1]\n",
    "\tfor sample in data:\n",
    "\t\ttotal = total + np.square(sample - mean_features)\n",
    "\n",
    "\tstd_features = np.divide(total, num_elements)\n",
    "\n",
    "\tvar_features = std_features ** 2\n",
    "\n",
    "\treturn mean_features, var_features\n",
    "\n",
    "# Normalize data by subtracting mean and dividing by standard deviation\n",
    "def normalize_data(data):\n",
    "\tmean_features, var_features = compute_mean_and_var(data)\n",
    "\tstd_features = np.sqrt(var_features)\n",
    "\n",
    "\tfor index, sample in enumerate(data):\n",
    "\t\tdata[index] = np.divide((sample - mean_features), std_features) \n",
    "\n",
    "\treturn data\n",
    "\n",
    "# Divide dataset based on if sample value on feature index is larger than\n",
    "# the given threshold\n",
    "def divide_on_feature(X, feature_i, threshold):\n",
    "    split_func = None\n",
    "    if isinstance(threshold, int) or isinstance(threshold, float):\n",
    "        split_func = lambda sample: sample[feature_i] >= threshold\n",
    "    else:\n",
    "        split_func = lambda sample: sample[feature_i] == threshold\n",
    "\n",
    "    X_1 = np.array([sample for sample in X if split_func(sample)])\n",
    "    X_2 = np.array([sample for sample in X if not split_func(sample)])\n",
    "\n",
    "    return np.array([X_1, X_2])\n",
    "\n",
    "# Return random subsets (with replacements) of the data\n",
    "def get_random_subsets(X, y, n_subsets, replacements=True):\n",
    "    n_samples = np.shape(X)[0]\n",
    "    # Concatenate x and y and do a random shuffle\n",
    "    X_y = np.concatenate((X, y.reshape((1, len(y))).T), axis=1)\n",
    "    np.random.shuffle(X_y)\n",
    "    subsets = []\n",
    "\n",
    "    # Uses 50% of training samples without replacements\n",
    "    subsample_size = n_samples // 2\n",
    "    if replacements:\n",
    "        subsample_size = n_samples      # 100% with replacements\n",
    "\n",
    "    for _ in range(n_subsets):\n",
    "        idx = np.random.choice(range(n_samples), size=np.shape(range(subsample_size)), replace=replacements)\n",
    "        X = X_y[idx][:, :-1]\n",
    "        y = X_y[idx][:, -1]\n",
    "        subsets.append([X, y])\n",
    "    return subsets\n",
    "\n",
    "# Calculate the entropy of label array y\n",
    "def calculate_entropy(y):\n",
    "    log2 = lambda x: np.log(x) / np.log(2)\n",
    "    unique_labels = np.unique(y)\n",
    "    entropy = 0\n",
    "    for label in unique_labels:\n",
    "        count = len(y[y == label])\n",
    "        p = count / len(y)\n",
    "        entropy += -p * log2(p)\n",
    "    return entropy\n",
    "\n",
    "# Returns the mean squared error between y_true and y_pred\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    mse = np.mean(np.power(y_true - y_pred, 2))\n",
    "    return mse\n",
    "\n",
    "# The sigmoid function\n",
    "def sigmoid(val):\n",
    "\treturn np.divide(1, (1 + np.exp(-1*val)))\n",
    "\n",
    "# The derivative of the sigmoid function\n",
    "def sigmoid_gradient(val):\n",
    "    return sigmoid(val) * (1 - sigmoid(val))\n",
    "\n",
    "# Compute the covariance matrix of an array\n",
    "def compute_cov_mat(data):\n",
    "\t# Compute the mean of the data\n",
    "\tmean_vec = np.mean(data, axis=0)\n",
    "\n",
    "\t# Compute the covariance matrix\n",
    "\tcov_mat = (data - mean_vec).T.dot((data - mean_vec)) / (data.shape[0]-1)\n",
    "\n",
    "\treturn cov_mat\n",
    "\n",
    "\n",
    "# Perform PCA dimensionality reduction\n",
    "def pca(data, exp_var_percentage=95):\n",
    "\n",
    "\t# Compute the covariance matrix\n",
    "\tcov_mat = compute_cov_mat(data)\n",
    "\n",
    "\t# Compute the eigen values and vectors of the covariance matrix\n",
    "\teig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
    "\n",
    "\t# Make a list of (eigenvalue, eigenvector) tuples\n",
    "\teig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "\n",
    "\t# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "\teig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "\t# Only keep a certain number of eigen vectors based on the \"explained variance percentage\"\n",
    "\t# which tells us how much information (variance) can be attributed to each of the principal components\n",
    "\ttot = sum(eig_vals)\n",
    "\tvar_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]\n",
    "\tcum_var_exp = np.cumsum(var_exp)\n",
    "\n",
    "\tnum_vec_to_keep = 0\n",
    "\n",
    "\tfor index, percentage in enumerate(cum_var_exp):\n",
    "\t\tif percentage > exp_var_percentage:\n",
    "\t\t\tnum_vec_to_keep = index + 1\n",
    "\t\t\tbreak\n",
    "\n",
    "\t# Compute the projection matrix based on the top eigen vectors\n",
    "\tproj_mat = eig_pairs[0][1].reshape(4,1)\n",
    "\tfor eig_vec_idx in range(1, num_vec_to_keep):\n",
    "\t\tproj_mat = np.hstack((proj_mat, eig_pairs[eig_vec_idx][1].reshape(4,1)))\n",
    "\n",
    "\t# Project the data \n",
    "\tpca_data = data.dot(proj_mat)\n",
    "\n",
    "\treturn pca_data\n",
    "\n",
    "# 1D Gaussian Function\n",
    "def gaussian_1d(val, mean, standard_dev):\n",
    "\tcoeff = 1 / (standard_dev * np.sqrt(2 * np.pi))\n",
    "\texponent = (-1 * (val - mean) ** 2) / (2 * (standard_dev ** 2))\n",
    "\tgauss = coeff * np.exp(exponent)\n",
    "\treturn gauss\n",
    "\n",
    "# 2D Gaussian Function\n",
    "def gaussian_2d(x_val, y_val, x_mean, y_mean, x_standard_dev, y_standard_dev):\n",
    "\tx_gauss = gaussian_1d(x_val, x_mean, x_standard_dev)\n",
    "\ty_gauss = gaussian_1d(y_val, y_mean, y_standard_dev)\n",
    "\tgauss = x_gauss * y_gauss\n",
    "\treturn gauss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.16439105\n",
      "Iteration 2, loss = 1.15909864\n",
      "Iteration 3, loss = 1.15162311\n",
      "Iteration 4, loss = 1.14265447\n",
      "Iteration 5, loss = 1.13262386\n",
      "Iteration 6, loss = 1.12208074\n",
      "Iteration 7, loss = 1.11160413\n",
      "Iteration 8, loss = 1.10121878\n",
      "Iteration 9, loss = 1.09105872\n",
      "Iteration 10, loss = 1.08132157\n",
      "Iteration 11, loss = 1.07204478\n",
      "Iteration 12, loss = 1.06312580\n",
      "Iteration 13, loss = 1.05464706\n",
      "Iteration 14, loss = 1.04673644\n",
      "Iteration 15, loss = 1.03968299\n",
      "Iteration 16, loss = 1.03348585\n",
      "Iteration 17, loss = 1.02801260\n",
      "Iteration 18, loss = 1.02329554\n",
      "Iteration 19, loss = 1.01918575\n",
      "Iteration 20, loss = 1.01558593\n",
      "Iteration 21, loss = 1.01233128\n",
      "Iteration 22, loss = 1.00940803\n",
      "Iteration 23, loss = 1.00671309\n",
      "Iteration 24, loss = 1.00413222\n",
      "Iteration 25, loss = 1.00157002\n",
      "Iteration 26, loss = 0.99898708\n",
      "Iteration 27, loss = 0.99638422\n",
      "Iteration 28, loss = 0.99374736\n",
      "Iteration 29, loss = 0.99110734\n",
      "Iteration 30, loss = 0.98855231\n",
      "Iteration 31, loss = 0.98599864\n",
      "Iteration 32, loss = 0.98348562\n",
      "Iteration 33, loss = 0.98106701\n",
      "Iteration 34, loss = 0.97874347\n",
      "Iteration 35, loss = 0.97643553\n",
      "Iteration 36, loss = 0.97415301\n",
      "Iteration 37, loss = 0.97189586\n",
      "Iteration 38, loss = 0.96967768\n",
      "Iteration 39, loss = 0.96761926\n",
      "Iteration 40, loss = 0.96562115\n",
      "Iteration 41, loss = 0.96363226\n",
      "Iteration 42, loss = 0.96167985\n",
      "Iteration 43, loss = 0.95974500\n",
      "Iteration 44, loss = 0.95782914\n",
      "Iteration 45, loss = 0.95592886\n",
      "Iteration 46, loss = 0.95402256\n",
      "Iteration 47, loss = 0.95212594\n",
      "Iteration 48, loss = 0.95024883\n",
      "Iteration 49, loss = 0.94840285\n",
      "Iteration 50, loss = 0.94656565\n",
      "Iteration 51, loss = 0.94479794\n",
      "Iteration 52, loss = 0.94317214\n",
      "Iteration 53, loss = 0.94168552\n",
      "Iteration 54, loss = 0.94029720\n",
      "Iteration 55, loss = 0.93893131\n",
      "Iteration 56, loss = 0.93760332\n",
      "Iteration 57, loss = 0.93635642\n",
      "Iteration 58, loss = 0.93517080\n",
      "Iteration 59, loss = 0.93400034\n",
      "Iteration 60, loss = 0.93288029\n",
      "Iteration 61, loss = 0.93177600\n",
      "Iteration 62, loss = 0.93069198\n",
      "Iteration 63, loss = 0.92964423\n",
      "Iteration 64, loss = 0.92860911\n",
      "Iteration 65, loss = 0.92758968\n",
      "Iteration 66, loss = 0.92657223\n",
      "Iteration 67, loss = 0.92553674\n",
      "Iteration 68, loss = 0.92449980\n",
      "Iteration 69, loss = 0.92346588\n",
      "Iteration 70, loss = 0.92242363\n",
      "Iteration 71, loss = 0.92138533\n",
      "Iteration 72, loss = 0.92035129\n",
      "Iteration 73, loss = 0.91931840\n",
      "Iteration 74, loss = 0.91829000\n",
      "Iteration 75, loss = 0.91726728\n",
      "Iteration 76, loss = 0.91624618\n",
      "Iteration 77, loss = 0.91523214\n",
      "Iteration 78, loss = 0.91422537\n",
      "Iteration 79, loss = 0.91322704\n",
      "Iteration 80, loss = 0.91224052\n",
      "Iteration 81, loss = 0.91125927\n",
      "Iteration 82, loss = 0.91026986\n",
      "Iteration 83, loss = 0.90928474\n",
      "Iteration 84, loss = 0.90830195\n",
      "Iteration 85, loss = 0.90732902\n",
      "Iteration 86, loss = 0.90635823\n",
      "Iteration 87, loss = 0.90539093\n",
      "Iteration 88, loss = 0.90442339\n",
      "Iteration 89, loss = 0.90345651\n",
      "Iteration 90, loss = 0.90247782\n",
      "Iteration 91, loss = 0.90149976\n",
      "Iteration 92, loss = 0.90052181\n",
      "Iteration 93, loss = 0.89954265\n",
      "Iteration 94, loss = 0.89855378\n",
      "Iteration 95, loss = 0.89756686\n",
      "Iteration 96, loss = 0.89658148\n",
      "Iteration 97, loss = 0.89559192\n",
      "Iteration 98, loss = 0.89460322\n",
      "Iteration 99, loss = 0.89362116\n",
      "Iteration 100, loss = 0.89266468\n",
      "Iteration 101, loss = 0.89171680\n",
      "Iteration 102, loss = 0.89076422\n",
      "Iteration 103, loss = 0.88981621\n",
      "Iteration 104, loss = 0.88887198\n",
      "Iteration 105, loss = 0.88793325\n",
      "Iteration 106, loss = 0.88699217\n",
      "Iteration 107, loss = 0.88605392\n",
      "Iteration 108, loss = 0.88511351\n",
      "Iteration 109, loss = 0.88417384\n",
      "Iteration 110, loss = 0.88323951\n",
      "Iteration 111, loss = 0.88231150\n",
      "Iteration 112, loss = 0.88138590\n",
      "Iteration 113, loss = 0.88046689\n",
      "Iteration 114, loss = 0.87955346\n",
      "Iteration 115, loss = 0.87863639\n",
      "Iteration 116, loss = 0.87773030\n",
      "Iteration 117, loss = 0.87682333\n",
      "Iteration 118, loss = 0.87592292\n",
      "Iteration 119, loss = 0.87503285\n",
      "Iteration 120, loss = 0.87414416\n",
      "Iteration 121, loss = 0.87325774\n",
      "Iteration 122, loss = 0.87237612\n",
      "Iteration 123, loss = 0.87150565\n",
      "Iteration 124, loss = 0.87064537\n",
      "Iteration 125, loss = 0.86979028\n",
      "Iteration 126, loss = 0.86893801\n",
      "Iteration 127, loss = 0.86809763\n",
      "Iteration 128, loss = 0.86725846\n",
      "Iteration 129, loss = 0.86643302\n",
      "Iteration 130, loss = 0.86560677\n",
      "Iteration 131, loss = 0.86479483\n",
      "Iteration 132, loss = 0.86398068\n",
      "Iteration 133, loss = 0.86318622\n",
      "Iteration 134, loss = 0.86239940\n",
      "Iteration 135, loss = 0.86162379\n",
      "Iteration 136, loss = 0.86085492\n",
      "Iteration 137, loss = 0.86010063\n",
      "Iteration 138, loss = 0.85934731\n",
      "Iteration 139, loss = 0.85860991\n",
      "Iteration 140, loss = 0.85785084\n",
      "Iteration 141, loss = 0.85709045\n",
      "Iteration 142, loss = 0.85632936\n",
      "Iteration 143, loss = 0.85558641\n",
      "Iteration 144, loss = 0.85483852\n",
      "Iteration 145, loss = 0.85410293\n",
      "Iteration 146, loss = 0.85337850\n",
      "Iteration 147, loss = 0.85266690\n",
      "Iteration 148, loss = 0.85196033\n",
      "Iteration 149, loss = 0.85125060\n",
      "Iteration 150, loss = 0.85053117\n",
      "Iteration 151, loss = 0.84982710\n",
      "Iteration 152, loss = 0.84913587\n",
      "Iteration 153, loss = 0.84845050\n",
      "Iteration 154, loss = 0.84777370\n",
      "Iteration 155, loss = 0.84708666\n",
      "Iteration 156, loss = 0.84641608\n",
      "Iteration 157, loss = 0.84575155\n",
      "Iteration 158, loss = 0.84510121\n",
      "Iteration 159, loss = 0.84445461\n",
      "Iteration 160, loss = 0.84381626\n",
      "Iteration 161, loss = 0.84318275\n",
      "Iteration 162, loss = 0.84255583\n",
      "Iteration 163, loss = 0.84194652\n",
      "Iteration 164, loss = 0.84136255\n",
      "Iteration 165, loss = 0.84079826\n",
      "Iteration 166, loss = 0.84023531\n",
      "Iteration 167, loss = 0.83968203\n",
      "Iteration 168, loss = 0.83913186\n",
      "Iteration 169, loss = 0.83859534\n",
      "Iteration 170, loss = 0.83806998\n",
      "Iteration 171, loss = 0.83756366\n",
      "Iteration 172, loss = 0.83705721\n",
      "Iteration 173, loss = 0.83656272\n",
      "Iteration 174, loss = 0.83607618\n",
      "Iteration 175, loss = 0.83559498\n",
      "Iteration 176, loss = 0.83512370\n",
      "Iteration 177, loss = 0.83465851\n",
      "Iteration 178, loss = 0.83420123\n",
      "Iteration 179, loss = 0.83375237\n",
      "Iteration 180, loss = 0.83330672\n",
      "Iteration 181, loss = 0.83286937\n",
      "Iteration 182, loss = 0.83243857\n",
      "Iteration 183, loss = 0.83201557\n",
      "Iteration 184, loss = 0.83159277\n",
      "Iteration 185, loss = 0.83117346\n",
      "Iteration 186, loss = 0.83075889\n",
      "Iteration 187, loss = 0.83034924\n",
      "Iteration 188, loss = 0.82993470\n",
      "Iteration 189, loss = 0.82952412\n",
      "Iteration 190, loss = 0.82911760\n",
      "Iteration 191, loss = 0.82871145\n",
      "Iteration 192, loss = 0.82831087\n",
      "Iteration 193, loss = 0.82791482\n",
      "Iteration 194, loss = 0.82752469\n",
      "Iteration 195, loss = 0.82713864\n",
      "Iteration 196, loss = 0.82675326\n",
      "Iteration 197, loss = 0.82637390\n",
      "Iteration 198, loss = 0.82599411\n",
      "Iteration 199, loss = 0.82562283\n",
      "Iteration 200, loss = 0.82525347\n",
      "Iteration 201, loss = 0.82488725\n",
      "Iteration 202, loss = 0.82452453\n",
      "Iteration 203, loss = 0.82416535\n",
      "Iteration 204, loss = 0.82380999\n",
      "Iteration 205, loss = 0.82345797\n",
      "Iteration 206, loss = 0.82310599\n",
      "Iteration 207, loss = 0.82277440\n",
      "Iteration 208, loss = 0.82245311\n",
      "Iteration 209, loss = 0.82213545\n",
      "Iteration 210, loss = 0.82182131\n",
      "Iteration 211, loss = 0.82150499\n",
      "Iteration 212, loss = 0.82119102\n",
      "Iteration 213, loss = 0.82087756\n",
      "Iteration 214, loss = 0.82056218\n",
      "Iteration 215, loss = 0.82025118\n",
      "Iteration 216, loss = 0.81993844\n",
      "Iteration 217, loss = 0.81962885\n",
      "Iteration 218, loss = 0.81932842\n",
      "Iteration 219, loss = 0.81902690\n",
      "Iteration 220, loss = 0.81873149\n",
      "Iteration 221, loss = 0.81843129\n",
      "Iteration 222, loss = 0.81813946\n",
      "Iteration 223, loss = 0.81784999\n",
      "Iteration 224, loss = 0.81756172\n",
      "Iteration 225, loss = 0.81727871\n",
      "Iteration 226, loss = 0.81699656\n",
      "Iteration 227, loss = 0.81671385\n",
      "Iteration 228, loss = 0.81643202\n",
      "Iteration 229, loss = 0.81614568\n",
      "Iteration 230, loss = 0.81586278\n",
      "Iteration 231, loss = 0.81557693\n",
      "Iteration 232, loss = 0.81528959\n",
      "Iteration 233, loss = 0.81499397\n",
      "Iteration 234, loss = 0.81469316\n",
      "Iteration 235, loss = 0.81439362\n",
      "Iteration 236, loss = 0.81409829\n",
      "Iteration 237, loss = 0.81380026\n",
      "Iteration 238, loss = 0.81350371\n",
      "Iteration 239, loss = 0.81321382\n",
      "Iteration 240, loss = 0.81292880\n",
      "Iteration 241, loss = 0.81264060\n",
      "Iteration 242, loss = 0.81235362\n",
      "Iteration 243, loss = 0.81206977\n",
      "Iteration 244, loss = 0.81178865\n",
      "Iteration 245, loss = 0.81150649\n",
      "Iteration 246, loss = 0.81122463\n",
      "Iteration 247, loss = 0.81096106\n",
      "Iteration 248, loss = 0.81070115\n",
      "Iteration 249, loss = 0.81044110\n",
      "Iteration 250, loss = 0.81018121\n",
      "Iteration 251, loss = 0.80992132\n",
      "Iteration 252, loss = 0.80966385\n",
      "Iteration 253, loss = 0.80940627\n",
      "Iteration 254, loss = 0.80915595\n",
      "Iteration 255, loss = 0.80890773\n",
      "Iteration 256, loss = 0.80865926\n",
      "Iteration 257, loss = 0.80840841\n",
      "Iteration 258, loss = 0.80815798\n",
      "Iteration 259, loss = 0.80790780\n",
      "Iteration 260, loss = 0.80765820\n",
      "Iteration 261, loss = 0.80741291\n",
      "Iteration 262, loss = 0.80716697\n",
      "Iteration 263, loss = 0.80691605\n",
      "Iteration 264, loss = 0.80667253\n",
      "Iteration 265, loss = 0.80641856\n",
      "Iteration 266, loss = 0.80616619\n",
      "Iteration 267, loss = 0.80592215\n",
      "Iteration 268, loss = 0.80567692\n",
      "Iteration 269, loss = 0.80543301\n",
      "Iteration 270, loss = 0.80518969\n",
      "Iteration 271, loss = 0.80494606\n",
      "Iteration 272, loss = 0.80470271\n",
      "Iteration 273, loss = 0.80446488\n",
      "Iteration 274, loss = 0.80422462\n",
      "Iteration 275, loss = 0.80398109\n",
      "Iteration 276, loss = 0.80374391\n",
      "Iteration 277, loss = 0.80350187\n",
      "Iteration 278, loss = 0.80326550\n",
      "Iteration 279, loss = 0.80303339\n",
      "Iteration 280, loss = 0.80280232\n",
      "Iteration 281, loss = 0.80257155\n",
      "Iteration 282, loss = 0.80234229\n",
      "Iteration 283, loss = 0.80210959\n",
      "Iteration 284, loss = 0.80188214\n",
      "Iteration 285, loss = 0.80165027\n",
      "Iteration 286, loss = 0.80142155\n",
      "Iteration 287, loss = 0.80119117\n",
      "Iteration 288, loss = 0.80096297\n",
      "Iteration 289, loss = 0.80073200\n",
      "Iteration 290, loss = 0.80050209\n",
      "Iteration 291, loss = 0.80026970\n",
      "Iteration 292, loss = 0.80003919\n",
      "Iteration 293, loss = 0.79981355\n",
      "Iteration 294, loss = 0.79958288\n",
      "Iteration 295, loss = 0.79935004\n",
      "Iteration 296, loss = 0.79912002\n",
      "Iteration 297, loss = 0.79889808\n",
      "Iteration 298, loss = 0.79866740\n",
      "Iteration 299, loss = 0.79843911\n",
      "Iteration 300, loss = 0.79821375\n",
      "Iteration 301, loss = 0.79798606\n",
      "Iteration 302, loss = 0.79775303\n",
      "Iteration 303, loss = 0.79752366\n",
      "Iteration 304, loss = 0.79728821\n",
      "Iteration 305, loss = 0.79705707\n",
      "Iteration 306, loss = 0.79681931\n",
      "Iteration 307, loss = 0.79658431\n",
      "Iteration 308, loss = 0.79635258\n",
      "Iteration 309, loss = 0.79613044\n",
      "Iteration 310, loss = 0.79590187\n",
      "Iteration 311, loss = 0.79567621\n",
      "Iteration 312, loss = 0.79544414\n",
      "Iteration 313, loss = 0.79521404\n",
      "Iteration 314, loss = 0.79497814\n",
      "Iteration 315, loss = 0.79474375\n",
      "Iteration 316, loss = 0.79450654\n",
      "Iteration 317, loss = 0.79426360\n",
      "Iteration 318, loss = 0.79402384\n",
      "Iteration 319, loss = 0.79378685\n",
      "Iteration 320, loss = 0.79355056\n",
      "Iteration 321, loss = 0.79330916\n",
      "Iteration 322, loss = 0.79306582\n",
      "Iteration 323, loss = 0.79282889\n",
      "Iteration 324, loss = 0.79258300\n",
      "Iteration 325, loss = 0.79234383\n",
      "Iteration 326, loss = 0.79209748\n",
      "Iteration 327, loss = 0.79185860\n",
      "Iteration 328, loss = 0.79161566\n",
      "Iteration 329, loss = 0.79137226\n",
      "Iteration 330, loss = 0.79113429\n",
      "Iteration 331, loss = 0.79089668\n",
      "Iteration 332, loss = 0.79065900\n",
      "Iteration 333, loss = 0.79042032\n",
      "Iteration 334, loss = 0.79018440\n",
      "Iteration 335, loss = 0.78994915\n",
      "Iteration 336, loss = 0.78970693\n",
      "Iteration 337, loss = 0.78947128\n",
      "Iteration 338, loss = 0.78922828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 339, loss = 0.78898864\n",
      "Iteration 340, loss = 0.78874717\n",
      "Iteration 341, loss = 0.78850295\n",
      "Iteration 342, loss = 0.78825987\n",
      "Iteration 343, loss = 0.78802540\n",
      "Iteration 344, loss = 0.78777319\n",
      "Iteration 345, loss = 0.78752960\n",
      "Iteration 346, loss = 0.78728823\n",
      "Iteration 347, loss = 0.78704843\n",
      "Iteration 348, loss = 0.78680045\n",
      "Iteration 349, loss = 0.78655332\n",
      "Iteration 350, loss = 0.78630448\n",
      "Iteration 351, loss = 0.78606024\n",
      "Iteration 352, loss = 0.78581494\n",
      "Iteration 353, loss = 0.78556645\n",
      "Iteration 354, loss = 0.78531918\n",
      "Iteration 355, loss = 0.78506121\n",
      "Iteration 356, loss = 0.78480254\n",
      "Iteration 357, loss = 0.78453712\n",
      "Iteration 358, loss = 0.78428188\n",
      "Iteration 359, loss = 0.78401808\n",
      "Iteration 360, loss = 0.78375854\n",
      "Iteration 361, loss = 0.78349438\n",
      "Iteration 362, loss = 0.78322429\n",
      "Iteration 363, loss = 0.78296891\n",
      "Iteration 364, loss = 0.78272090\n",
      "Iteration 365, loss = 0.78246206\n",
      "Iteration 366, loss = 0.78220971\n",
      "Iteration 367, loss = 0.78196119\n",
      "Iteration 368, loss = 0.78170103\n",
      "Iteration 369, loss = 0.78144069\n",
      "Iteration 370, loss = 0.78119469\n",
      "Iteration 371, loss = 0.78094041\n",
      "Iteration 372, loss = 0.78068005\n",
      "Iteration 373, loss = 0.78040104\n",
      "Iteration 374, loss = 0.78010433\n",
      "Iteration 375, loss = 0.77981484\n",
      "Iteration 376, loss = 0.77953058\n",
      "Iteration 377, loss = 0.77923529\n",
      "Iteration 378, loss = 0.77894992\n",
      "Iteration 379, loss = 0.77865688\n",
      "Iteration 380, loss = 0.77836388\n",
      "Iteration 381, loss = 0.77807248\n",
      "Iteration 382, loss = 0.77777416\n",
      "Iteration 383, loss = 0.77748380\n",
      "Iteration 384, loss = 0.77719697\n",
      "Iteration 385, loss = 0.77690619\n",
      "Iteration 386, loss = 0.77662078\n",
      "Iteration 387, loss = 0.77633297\n",
      "Iteration 388, loss = 0.77604988\n",
      "Iteration 389, loss = 0.77576122\n",
      "Iteration 390, loss = 0.77548009\n",
      "Iteration 391, loss = 0.77519445\n",
      "Iteration 392, loss = 0.77491214\n",
      "Iteration 393, loss = 0.77462709\n",
      "Iteration 394, loss = 0.77433808\n",
      "Iteration 395, loss = 0.77405352\n",
      "Iteration 396, loss = 0.77376789\n",
      "Iteration 397, loss = 0.77348889\n",
      "Iteration 398, loss = 0.77321779\n",
      "Iteration 399, loss = 0.77293519\n",
      "Iteration 400, loss = 0.77265231\n",
      "Iteration 401, loss = 0.77236969\n",
      "Iteration 402, loss = 0.77209441\n",
      "Iteration 403, loss = 0.77180947\n",
      "Iteration 404, loss = 0.77153292\n",
      "Iteration 405, loss = 0.77125051\n",
      "Iteration 406, loss = 0.77097205\n",
      "Iteration 407, loss = 0.77069443\n",
      "Iteration 408, loss = 0.77041473\n",
      "Iteration 409, loss = 0.77013186\n",
      "Iteration 410, loss = 0.76985918\n",
      "Iteration 411, loss = 0.76958380\n",
      "Iteration 412, loss = 0.76931217\n",
      "Iteration 413, loss = 0.76904256\n",
      "Iteration 414, loss = 0.76877078\n",
      "Iteration 415, loss = 0.76849567\n",
      "Iteration 416, loss = 0.76822596\n",
      "Iteration 417, loss = 0.76795034\n",
      "Iteration 418, loss = 0.76768204\n",
      "Iteration 419, loss = 0.76740522\n",
      "Iteration 420, loss = 0.76712904\n",
      "Iteration 421, loss = 0.76685797\n",
      "Iteration 422, loss = 0.76658325\n",
      "Iteration 423, loss = 0.76631218\n",
      "Iteration 424, loss = 0.76604217\n",
      "Iteration 425, loss = 0.76575801\n",
      "Iteration 426, loss = 0.76548590\n",
      "Iteration 427, loss = 0.76520570\n",
      "Iteration 428, loss = 0.76492651\n",
      "Iteration 429, loss = 0.76465187\n",
      "Iteration 430, loss = 0.76437292\n",
      "Iteration 431, loss = 0.76409034\n",
      "Iteration 432, loss = 0.76381240\n",
      "Iteration 433, loss = 0.76353241\n",
      "Iteration 434, loss = 0.76325087\n",
      "Iteration 435, loss = 0.76297217\n",
      "Iteration 436, loss = 0.76269006\n",
      "Iteration 437, loss = 0.76241002\n",
      "Iteration 438, loss = 0.76212629\n",
      "Iteration 439, loss = 0.76184946\n",
      "Iteration 440, loss = 0.76156046\n",
      "Iteration 441, loss = 0.76126586\n",
      "Iteration 442, loss = 0.76097060\n",
      "Iteration 443, loss = 0.76067813\n",
      "Iteration 444, loss = 0.76038387\n",
      "Iteration 445, loss = 0.76009983\n",
      "Iteration 446, loss = 0.75981418\n",
      "Iteration 447, loss = 0.75953406\n",
      "Iteration 448, loss = 0.75926415\n",
      "Iteration 449, loss = 0.75898396\n",
      "Iteration 450, loss = 0.75870864\n",
      "Iteration 451, loss = 0.75843812\n",
      "Iteration 452, loss = 0.75816201\n",
      "Iteration 453, loss = 0.75790142\n",
      "Iteration 454, loss = 0.75763087\n",
      "Iteration 455, loss = 0.75736514\n",
      "Iteration 456, loss = 0.75710294\n",
      "Iteration 457, loss = 0.75683727\n",
      "Iteration 458, loss = 0.75658223\n",
      "Iteration 459, loss = 0.75632183\n",
      "Iteration 460, loss = 0.75606482\n",
      "Iteration 461, loss = 0.75581087\n",
      "Iteration 462, loss = 0.75555353\n",
      "Iteration 463, loss = 0.75529875\n",
      "Iteration 464, loss = 0.75504824\n",
      "Iteration 465, loss = 0.75480362\n",
      "Iteration 466, loss = 0.75455972\n",
      "Iteration 467, loss = 0.75431684\n",
      "Iteration 468, loss = 0.75406447\n",
      "Iteration 469, loss = 0.75382116\n",
      "Iteration 470, loss = 0.75358129\n",
      "Iteration 471, loss = 0.75333603\n",
      "Iteration 472, loss = 0.75308454\n",
      "Iteration 473, loss = 0.75285671\n",
      "Iteration 474, loss = 0.75259836\n",
      "Iteration 475, loss = 0.75235345\n",
      "Iteration 476, loss = 0.75210800\n",
      "Iteration 477, loss = 0.75185902\n",
      "Iteration 478, loss = 0.75160566\n",
      "Iteration 479, loss = 0.75136667\n",
      "Iteration 480, loss = 0.75111269\n",
      "Iteration 481, loss = 0.75086759\n",
      "Iteration 482, loss = 0.75061505\n",
      "Iteration 483, loss = 0.75037671\n",
      "Iteration 484, loss = 0.75011454\n",
      "Iteration 485, loss = 0.74987918\n",
      "Iteration 486, loss = 0.74963445\n",
      "Iteration 487, loss = 0.74939320\n",
      "Iteration 488, loss = 0.74915575\n",
      "Iteration 489, loss = 0.74891419\n",
      "Iteration 490, loss = 0.74867735\n",
      "Iteration 491, loss = 0.74843965\n",
      "Iteration 492, loss = 0.74820082\n",
      "Iteration 493, loss = 0.74796317\n",
      "Iteration 494, loss = 0.74772941\n",
      "Iteration 495, loss = 0.74748794\n",
      "Iteration 496, loss = 0.74725562\n",
      "Iteration 497, loss = 0.74702178\n",
      "Iteration 498, loss = 0.74678337\n",
      "Iteration 499, loss = 0.74654655\n",
      "Iteration 500, loss = 0.74630507\n",
      "Classification Accuracy =  0.6666666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:568: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(solver='sgd', learning_rate_init=0.01, momentum=0.9, early_stopping=False, learning_rate='constant', hidden_layer_sizes=(15, 10, 5), max_iter=500, verbose=True)\n",
    "\n",
    "# Get the training data\n",
    "# Import the Iris flower dataset\n",
    "iris = datasets.load_iris()\n",
    "train_data = np.array(iris.data)\n",
    "train_labels = np.array(iris.target)\n",
    "num_features = train_data.data.shape[1]\n",
    "\n",
    "# Randomly shuffle the data\n",
    "train_data, train_labels = shuffle_data(train_data, train_labels)\n",
    "\n",
    "# Normalize the training data\n",
    "train_data = normalize_data(train_data)\n",
    "\n",
    "# Train the Neural Network on the data\n",
    "clf.fit(train_data, train_labels)\n",
    "\n",
    "# Compute the training accuracy\n",
    "Accuracy = 0\n",
    "for index in range(len(train_labels)):\n",
    "\tcurrent_sample = train_data[index].reshape(1,-1) \n",
    "\tcurrent_label = train_labels[index]\n",
    "\tpredicted_label = clf.predict(current_sample)\n",
    "\n",
    "\tif current_label == predicted_label:\n",
    "\t\tAccuracy += 1\n",
    "\n",
    "Accuracy /= len(train_labels)\n",
    "# Print stuff\n",
    "print(\"Classification Accuracy = \", Accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
